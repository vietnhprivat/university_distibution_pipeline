{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collecting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection by Webscraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Making a script that Search through the url and then locating the download .xlsx / .xls url and then renaming the file and downloading it in the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viet Nguyen\\AppData\\Local\\Temp\\ipykernel_8364\\2136602830.py:12: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  h2_tag = soup.find('h2', text=h2_text)\n",
      "C:\\Users\\Viet Nguyen\\AppData\\Local\\Temp\\ipykernel_8364\\2136602830.py:13: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  h3_tag = h2_tag.find_next('h3', text=h3_text)\n"
     ]
    }
   ],
   "source": [
    "# Function to create a folder if it does not exist\n",
    "def create_folder(folder_name):\n",
    "    try:\n",
    "        os.makedirs(folder_name, exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating folder: {folder_name} - {e}\")\n",
    "\n",
    "# Function to download and save files based on the specified headers and year range\n",
    "def download_and_save_files(h2_text, h3_text, folder_name, file_prefix, years):\n",
    "     \n",
    "    # Find the relevant h2 and h3 tags\n",
    "    h2_tag = soup.find('h2', text=h2_text)\n",
    "    h3_tag = h2_tag.find_next('h3', text=h3_text)\n",
    "\n",
    "    # Find the <p> tag containing the file links\n",
    "    p_files = h3_tag.find_next('p')\n",
    "\n",
    "    # Iterate through the file links\n",
    "    for link in p_files.find_all('a'):\n",
    "        # Check if the year of the file is in the desired range\n",
    "        try:\n",
    "            year = int(link.text.strip())\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        if year not in years:\n",
    "            continue\n",
    "\n",
    "        # Construct the absolute file URL\n",
    "        file_url = urljoin(url, link['href'])\n",
    "\n",
    "        # Extract the file name from the URL\n",
    "        parsed_url = urlparse(file_url)\n",
    "        file_name = os.path.basename(parsed_url.path)\n",
    "\n",
    "        # Extract the file extension from the file name\n",
    "        file_ext = os.path.splitext(file_name)[1]\n",
    "\n",
    "        # Construct the new file name with the format \"{file_prefix}_year.ext\"\n",
    "        new_file_name = f\"{file_prefix}_{year}{file_ext}\"\n",
    "\n",
    "        # Download the file and save it in the folder\n",
    "        file_response = requests.get(file_url)\n",
    "        with open(os.path.join(folder_name, new_file_name), 'wb') as file:\n",
    "            file.write(file_response.content)\n",
    "\n",
    "# Define the URL of the website you want to scrape\n",
    "url = 'https://ufm.dk/uddannelse/statistik-og-analyser/sogning-og-optag-pa-videregaende-uddannelser/grundtal-om-sogning-og-optag/ansogere-og-optagne-fordelt-pa-kon-alder-og-adgangsgrundlag/ansogere-og-optagne-fordelt-pa-kon-alder-og-adgangsgrundlag'\n",
    "\n",
    "# Send an HTTP request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Define the headers and sub-headers to iterate over\n",
    "    headers = [\n",
    "        ('Ansøgere', 'applicants_'),\n",
    "        ('Optagne', 'admitted_')\n",
    "    ]\n",
    "\n",
    "    sub_headers = [\n",
    "        ('Køn:', 'gender'),\n",
    "        #('Alder:', 'age'),\n",
    "        #('Adgangsgrundlag:', 'acceptance')\n",
    "    ]\n",
    "\n",
    "    # Define the range of years to download files for\n",
    "    years = range(2009, 2023)\n",
    "\n",
    "    # Loop over headers and sub-headers\n",
    "    for h2_text, h2_prefix in headers:\n",
    "        for h3_text, folder_name in sub_headers:\n",
    "\n",
    "            # Construct the file prefix using h2_prefix and folder_name\n",
    "            file_prefix = f\"{h2_prefix}{folder_name}\"\n",
    "\n",
    "            # Create the folder for storing files\n",
    "           \n",
    "            create_folder(folder_name)\n",
    "\n",
    "            # Download and save files\n",
    "            download_and_save_files(h2_text, h3_text, folder_name, file_prefix, years)\n",
    "else:\n",
    "    print(f'Failed to fetch the website: {url}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function that convers all .xls and .xlsx files to .csv \n",
    "def convert_to_csv_and_delete(folder_name):\n",
    "    for file in os.listdir(folder_name):\n",
    "        file_path = os.path.join(folder_name, file)\n",
    "\n",
    "        # Get the file extension\n",
    "        file_ext = os.path.splitext(file_path)[1]\n",
    "\n",
    "        # Check if the file has a .xls or .xlsx extension\n",
    "        if file_ext == '.xls' or file_ext == '.xlsx':\n",
    "            # Read the file using pandas\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "            # Construct the new file name with .csv extension\n",
    "            new_file_name = os.path.splitext(file)[0] + '.csv'\n",
    "\n",
    "            # Save the DataFrame as a CSV file\n",
    "            df.to_csv(os.path.join(folder_name, new_file_name), index=False)\n",
    "\n",
    "            # Delete the original .xls or .xlsx file\n",
    "            os.remove(file_path)\n",
    "\n",
    "# Convert .xls and .xlsx files to .csv format in the respective folders and delete the original files\n",
    "folders = ['gender', 'age', 'acceptance']\n",
    "for folder in folders:\n",
    "    convert_to_csv_and_delete(folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
